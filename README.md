# Educational Data Crawler

Dá»± Ã¡n crawl dá»¯ liá»‡u Ä‘á» thi vÃ  tÃ i liá»‡u giÃ¡o dá»¥c tá»« cÃ¡c nguá»“n trá»±c tuyáº¿n, sá»­ dá»¥ng Scrapy framework.

## ğŸ“‹ MÃ´ táº£

Crawler tá»± Ä‘á»™ng thu tháº­p vÃ  xá»­ lÃ½ dá»¯ liá»‡u Ä‘á» thi cÃ¡c mÃ´n há»c (ToÃ¡n, HÃ³a, STEM) tá»« website loigiaihay.com vÃ  cÃ¡c nguá»“n khÃ¡c. Dá»¯ liá»‡u Ä‘Æ°á»£c cáº¥u trÃºc hÃ³a vÃ  lÆ°u trá»¯ dÆ°á»›i dáº¡ng JSON, phÃ¹ há»£p cho cÃ¡c á»©ng dá»¥ng há»c táº­p vÃ  phÃ¢n tÃ­ch.

## ğŸ—‚ï¸ Cáº¥u trÃºc dá»± Ã¡n

```
crawler/
â”œâ”€â”€ crawler/
â”‚   â”œâ”€â”€ spiders/
â”‚   â”‚   â”œâ”€â”€ math_loigiaihay.py      # Spider crawl Ä‘á» thi ToÃ¡n
â”‚   â”‚   â”œâ”€â”€ stem_loigiaihay.py      # Spider crawl STEM (Khoa há»c tá»± nhiÃªn)
â”‚   â”‚   â”œâ”€â”€ ptit_crawler.py         # Spider crawl tin tá»©c PTIT
â”‚   â”‚   â””â”€â”€ vietjack.py             # Spider crawl tá»« Vietjack
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ get_collections.py      # HÃ m táº¡o collection links
â”‚   â”œâ”€â”€ items.py                    # Äá»‹nh nghÄ©a data items
â”‚   â”œâ”€â”€ middlewares.py              # Custom middlewares
â”‚   â”œâ”€â”€ pipelines.py                # Data processing pipelines
â”‚   â””â”€â”€ settings.py                 # Cáº¥u hÃ¬nh Scrapy
â”œâ”€â”€ pyproject.toml                  # Dependencies & project config
â”œâ”€â”€ scrapy.cfg                      # Scrapy configuration
â””â”€â”€ README.md
```

## ğŸš€ CÃ i Ä‘áº·t

### YÃªu cáº§u

- Python 3.11+
- pip hoáº·c uv package manager

### CÃ¡c bÆ°á»›c cÃ i Ä‘áº·t

1. Clone repository:
```bash
git clone <repository-url>
cd crawler
```

2. Táº¡o virtual environment:
```bash
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# hoáº·c
.venv\Scripts\activate     # Windows
```

3. CÃ i Ä‘áº·t dependencies:
```bash
pip install -r requirements.txt
# hoáº·c sá»­ dá»¥ng uv
uv sync
```

## ğŸ“– Sá»­ dá»¥ng

### Cháº¡y spider cÆ¡ báº£n

```bash
# Crawl Ä‘á» thi ToÃ¡n lá»›p 10
scrapy crawl math_loigiaihay -o math_10.json

# Crawl Ä‘á» thi STEM lá»›p 10
scrapy crawl stem_loigiaihay -o stem_10.json

# Crawl Ä‘á» thi HÃ³a lá»›p 10
scrapy crawl chemical_loigiaihay -o chemical_10.json
```

### Tuá»³ chá»‰nh output

```bash
# Export ra CSV
scrapy crawl math_loigiaihay -o output.csv

# Export ra JSON Lines
scrapy crawl math_loigiaihay -o output.jsonl

# Export vá»›i encoding cá»¥ thá»ƒ
scrapy crawl math_loigiaihay -o output.json -s FEED_EXPORT_ENCODING=utf-8
```

## ğŸ•·ï¸ Spiders

### 1. **math_loigiaihay**
- **Nguá»“n**: loigiaihay.com
- **Má»¥c Ä‘Ã­ch**: Crawl Ä‘á» thi ToÃ¡n 

### 2. **stem_loigiaihay**
- **Nguá»“n**: loigiaihay.com
- **Má»¥c Ä‘Ã­ch**: Crawl Ä‘á» thi Khoa há»c tá»± nhiÃªn 
- 
### 3. **chemical_loigiaihay**
- **Nguá»“n**: loigiaihay.com
- **Má»¥c Ä‘Ã­ch**: Crawl Ä‘á» thi HÃ³a há»c 

### 4. **ptit_crawler**
- **Nguá»“n**: ptit.edu.vn
- **Má»¥c Ä‘Ã­ch**: Crawl tin tá»©c vÃ  bÃ i viáº¿t tá»« PTIT

## ğŸ”§ Pipelines

### ExamPipeline
- Validate cÃ¡c trÆ°á»ng báº¯t buá»™c (question, answer, grade, subject)
- Loáº¡i bá» items chá»©a hÃ¬nh áº£nh markers
- XÃ³a prefix "CÃ¢u X:" khá»i cÃ¢u há»i
- LÃ m sáº¡ch vÃ  format dá»¯ liá»‡u

### StemPipeline
- TÆ°Æ¡ng tá»± ExamPipeline
- ThÃªm filter loáº¡i bá» cÃ¢u há»i chá»©a tá»« "báº£ng"
- XÃ³a cÃ¡c chuá»—i khÃ´ng cáº§n thiáº¿t (Pháº§n tá»± luáº­n, Háº¾T, etc.)

### PtitPipeline
- LÆ°u content vÃ o file text
- Tá»• chá»©c theo cáº¥u trÃºc thÆ° má»¥c

## âš™ï¸ Cáº¥u hÃ¬nh

### settings.py

```python
ROBOTSTXT_OBEY = True                    # TuÃ¢n thá»§ robots.txt
CONCURRENT_REQUESTS_PER_DOMAIN = 4       # Sá»‘ request Ä‘á»“ng thá»i
DOWNLOAD_DELAY = 2                       # Delay giá»¯a cÃ¡c request (giÃ¢y)
FEED_EXPORT_ENCODING = 'utf-8'          # Encoding cho output file
LOG_LEVEL = 'INFO'                       # Má»©c Ä‘á»™ logging
```

## ğŸ“ LÆ°u Ã½

1. **Rate limiting**: Crawler cÃ³ DOWNLOAD_DELAY = 2s Ä‘á»ƒ trÃ¡nh overload server
2. **Robots.txt**: Tá»± Ä‘á»™ng tuÃ¢n thá»§ quy táº¯c robots.txt
3. **Data quality**: Pipeline tá»± Ä‘á»™ng validate vÃ  filter dá»¯ liá»‡u
4. **Images**: Items chá»©a hÃ¬nh áº£nh sáº½ bá»‹ dropped (cÃ³ thá»ƒ tuá»³ chá»‰nh)

## ğŸ› ï¸ Development

### ThÃªm spider má»›i

```bash
scrapy genspider <spider_name> <domain>
```

### Test spider

```bash
# Test vá»›i 1 URL cá»¥ thá»ƒ
scrapy parse <url> --spider=<spider_name>

# Cháº¡y vá»›i debug mode
scrapy crawl <spider_name> -L DEBUG
```

### Xem log

```bash
tail -f scrapy_log.txt
```